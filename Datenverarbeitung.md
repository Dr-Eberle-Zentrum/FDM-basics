---
title: "Datenverarbeitung"
teaching: 30
exercises: 10
---

:::::::::::::::::::::::::::::::::::::: questions 

- Reproduzierbar?
- Dokumentiert?
- Skalierbar?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Gr√ºnde f√ºr den Einsatz von Workflow-Systemen und Skriptsprachen
- Verst√§ndnis von Pipelines und deren Aufbau
- Was, wenn der eigene Rechner nicht mehr reicht?


::::::::::::::::::::::::::::::::::::::::::::::::


Datenverarbeitung ist der Kernpunkt jedes Forschungsprozesses. 
In dieser Sitzung werden wir uns mit grundlegenden Aspekten der Datenverarbeitung im Kontext des Forschungsdatenmanagements (FDM) besch√§ftigen. 
Hierbei liegt der Fokus auf der Bedeutung von Workflow-Systemen, Skriptsprachen und Pipelines f√ºr die effiziente und reproduzierbare Arbeitsabl√§ufe.


Bevor wir starten, hier ein kleines fiktives Beispiel, um die Relevanz der Themen zu verdeutlichen:

::: tab

### Was bisher geschah ...

Sie versp√ºren heute einen unb√§ndigen Tatendrang und m√∂chten Daten zu Bl√ºtenformen analysieren.

Nach kurzer Recherche finden sie den [Iris-Datensatz](https://en.wikipedia.org/wiki/Iris_flower_data_set), der Informationen zu verschiedenen Iris-Arten und deren Bl√ºtenmerkmalen enth√§lt.
Dieser Datensatz ist in einer [CSV-Datei](https://zenodo.org/record/1319069/files/iris.csv) im [Zenodo Datenrepositorium](https://zenodo.org/records/1319069) verf√ºgbar, und sie laden ihn sofort herunter.

![Iris Bl√ºten verschiedener Arten (Autoren v.l.n.r: D. Langlois , C. Johansson, S. Keohane, [Lizenzinfos](https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101-everyone/tutorial.html#analysis-how-to-differentiate-the-different-iris-species))](https://training.galaxyproject.org/training-material/topics/introduction/images/iris_flowers.png){width=70% alt="Three species of Iris flowers"}

Da sie lieber im TSV-Format arbeiten, konvertieren sie die Datei mit einem Texteditor mit Hilfe der "Finden und Ersetzen"-Funktion in das gew√ºnschte Format.
Ausserdem l√∂schen sie die Kopfzeile mit den Spaltennamen, da sie diese nicht ben√∂tigen.

Nun m√∂chten sie die Daten analysieren.
Die erste Frage, die sich aufdr√§ngt, ist, wieviele verschiedene Iris-Arten im Datensatz enthalten sind.
Hierf√ºr erinnern sie sich an die "Spaltenselektion"-Funktion ihres Texteditors (Alt-Taste (Windows/Linux) bzw. Befehlstaste (Mac) gedr√ºckt halten und dann die Maus ziehen) und extrahieren die Spalte mit den Artennamen.
Diese kopieren sie in eine neue Datei und verwenden die "Duplikate entfernen"-Funktion ihres Texteditors, um die einzigartigen Arten zu identifizieren.
Wunderbar, es sind genau drei Arten, wie erwartet.

Danach bearbeiten sie die Daten weiter mit einer Kombination aus Texteditor und Tabellenkalkulationsprogramm, um herauszufinden,

- Wieviele Datens√§tze gibt es pro Art?
- Wie sind die durchschnittlichen Bl√ºtenma√üe pro Art?
- Wie verteilen sich die [Datenpunkte der Sepalbl√§tter in einem Streudiagramm](https://training.galaxyproject.org/training-material/topics/introduction/images/101_foreveryone_scatter.png) coloriert nach Art?

Am Ende ihres Arbeitstages sind sie zufrieden mit den Ergebnissen, aber auch etwas ersch√∂pft.
Sie haben sich auf einem Blatt Papier die Ergebniszahlen notiert und die Grafik ausgedruckt.

:::


:::::::::: challenge

## Und nun?

Ein paar Tage sp√§ter sprechen sie mit Freunden √ºber ihre Analyse und holen ihre Notizen und den Ausdruck.

Was k√∂nnte nun das Problem sein?

::: solution

## M√∂gliche Probleme

- **Nicht nachvollziehbar**: Leider sind sie sich nicht mehr sicher, welche Zahl auf ihren Notizen welche Art repr√§sentiert.
- **Nicht reproduzierbar**: Der Hamster hat den halben Ausdruck gefressen, aber sie wissen nicht mehr genau, wie sie die Grafik erstellt haben.
- **Fehleranf√§llig**: Beim Kopieren der Daten in den Texteditor haben sie versehentlich eine Zeile gel√∂scht, was die Ergebnisse verf√§lscht. Bzw. beim Notieren der Ergebniszahlen hatten sie einen Zahlendreher.
- **Zeitaufwand**: Die manuelle Bearbeitung der Daten hat viel Zeit in Anspruch genommen, die sie lieber in die Interpretation der Ergebnisse investiert h√§tten.
- **Fehlende Skalierbarkeit**: Sie m√∂chten die Analyse auf einen gr√∂√üeren Datensatz anwenden, aber die manuelle Methode ist nicht praktikabel f√ºr gr√∂√üere Datenmengen.

::::::::::::

::::::::::::::::::::


## Ziele guter Datenverarbeitung

Gute Datenverarbeitung sollte folgende Ziele verfolgen:


- **Reproduzierbarkeit**: Andere (oder sie selbst zu einem sp√§teren Zeitpunkt) sollten die Analyse mit den gleichen Daten und Methoden wiederholen k√∂nnen und zu den gleichen Ergebnissen kommen.
- **Nachvollziehbarkeit**: Jede Entscheidung und jeder Schritt in der Datenverarbeitung sollte dokumentiert sein, um die Analyse transparent zu machen.
- **Automatisierung**: Wiederkehrende Aufgaben sollten automatisiert werden, um menschliche Fehler zu minimieren und Zeit zu sparen.
- **Skalierbarkeit**: Die Methoden sollten auf gr√∂√üere Datens√§tze anwendbar sein, ohne dass der Aufwand exponentiell steigt.
- **Portabilit√§t**: Die Analyse sollte auf verschiedenen Systemen und Umgebungen durchf√ºhrbar sein, ohne dass umfangreiche Anpassungen erforderlich sind.

Reproduzierbarkeit und Nachvollziehbarkeit sind hierbei zwei zentrale Prinzipien in der wissenschaftlichen Forschung, die sicherstellen, dass Forschungsergebnisse verl√§sslich und √ºberpr√ºfbar sind.

Damit diese erreicht werden k√∂nnen, bedarf es entweder einer extrem umfangreichen Dokumentation oder - und das ist der bessere Weg - die Datenverarbeitung sollte mit Hilfe von Skriptsprachen und/oder Workflow-Systemen erfolgen.




## Skriptsprachen

Skriptsprachen sind Programmiersprachen, die speziell f√ºr die Automatisierung von Aufgaben und die Verarbeitung von Daten entwickelt wurden. 
Sie erm√∂glichen es Forschenden, komplexe Datenverarbeitungsaufgaben durch das Schreiben von Code zu automatisieren ohne dabei spezifische Kenntnisse √ºber computerinterne Abl√§ufe und Strukturen haben zu m√ºssen.
Das hei√üt, Skriptsprachen abstrahieren viele technische Details und bieten eine benutzerfreundliche Syntax, die es auch Nicht-Programmierern erm√∂glicht, effektive Datenverarbeitungsprozesse zu erstellen.
Zudem sind Skriptsprachen oft plattformunabh√§ngig, was bedeutet, dass Skripte auf verschiedenen Betriebssystemen ausgef√ºhrt werden k√∂nnen, ohne dass sie angepasst werden m√ºssen.

Verbreitete Skriptsprachen im Forschungsdatenmanagement sind:

- **Python**: Bekannt f√ºr seine Lesbarkeit und Vielseitigkeit, mit umfangreichen Bibliotheken f√ºr Datenanalyse (z.B. Pandas, NumPy).
- **R**: Speziell f√ºr statistische Analysen und Datenvisualisierung entwickelt, mit einer gro√üen Community und vielen Paketen (z.B. ggplot2, dplyr).
- **Bash**: Eine Unix-Shell und Skriptsprache, die h√§ufig f√ºr Systemadministration und Automatisierung von Aufgaben in Linux-Umgebungen verwendet wird.

Auch die mit der Webentwicklung verbundene Sprache *JavaScript* kann f√ºr Datenverarbeitungsaufgaben genutzt werden.

Wenngleich Skriptsprachen viele technische Details abstrahieren, ist es dennoch wichtig, ein grundlegendes Verst√§ndnis der zugrunde liegenden Konzepte zu haben, um effektiv und effizient arbeiten zu k√∂nnen.

Bei der Verwendung von Skriptsprachen muss unterschieden werden zwischen 

- **Interaktive Verarbeitung**, bei der einzelne Befehle oder Code-Snippets in einer interaktiven, chatartigen Eingabe ausgef√ºhrt werden, um Daten zu analysieren und zu visualisieren.
- **Automatisierte Verarbeitung**, bei der komplette Skripte geschrieben werden, die eine Reihe von Befehlen enthalten, die nacheinander ausgef√ºhrt werden, um eine vollst√§ndige Datenverarbeitungsaufgabe zu erledigen.

Die interaktive Verarbeitung eignet sich gut f√ºr explorative Datenanalyse und schnelle Tests, hat aber die gleichen Nachteile wie die manuelle Verarbeitung.
**Die automatisierte Verarbeitung ist daher zu bevorzugen und eigenet sich f√ºr wiederholbare und skalierbare Aufgaben.**

Einer der gr√∂√üten Vorteile von Skriptsprachen (im Rahmen der automatisieren Verarbeitung) ist es, dass sie direkt Dokumentation und Automatisierung verbinden und i.d.R. auch Skalierbarkeit und Portabilit√§t unterst√ºtzen.
Das hei√üt, ein einmal geschriebenes Skript kann immer wieder ausgef√ºhrt werden, ohne dass manuelle Schritte erforderlich sind.
Zudem erlaubt das Skript einzelne Schritte und Entscheidungen zu dokumentieren, was die Nachvollziehbarkeit erh√∂ht.

Im Folgenden haben wir das Datenverarbeitungsbeispiel von oben in einfache Python- und R-Skripte √ºbersetzt, welche die gleichen Schritte automatisiert durchf√ºhrt:

::::::::::::: tab


### R-Skript

Hier als R-Skript:

```R
library(tidyverse)  
# (1) Daten einlesen (ersetzt Umformatierung und Kopfzeilenentfernung)
data <- read_csv('https://zenodo.org/record/1319069/files/iris.csv')
# (2) Enthaltene Arten extrahieren
distinct(data, Species)
# (3) Anzahl der Datens√§tze pro Art bestimmen
count(data, Species)
# (4) Gruppieren pro Art und Mittelwerte der Spalten berechnen als Pipeline
data |> 
  group_by(Species) |> 
  summarise(across(everything(), mean))
# (5) Visualisierung als Streudiagram
ggplot(data, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() +
  labs(title='Sepal length as a function of sepal width', 
       x='Sepal length', y='Sepal width')
```

Der obige Code ist [online via rdrr.io ausf√ºhrbar](https://rdrr.io/snippets/embed/?code=library%28tidyverse%29%20%20%0A%23%20%281%29%20Daten%20laden%20%28ersetzt%20Import%2C%20Umformatierung%20und%20Kopfzeilenentfernung%29%0Adata%20%3C-%20iris%0A%23%20%282%29%20Enthaltene%20Arten%20extrahieren%0Adistinct%28data%2C%20Species%29%0A%23%20%283%29%20Anzahl%20der%20Datens%C3%A4tze%20pro%20Art%20bestimmen%0Acount%28data%2C%20Species%29%0A%23%20%284%29%20Gruppieren%20pro%20Art%20und%20Mittelwerte%20der%20Spalten%20berechnen%20als%20Pipeline%0Adata%20%7C%3E%20%0A%20%20group_by%28Species%29%20%7C%3E%20%0A%20%20summarise%28across%28everything%28%29%2C%20mean%29%29%0A%23%20%285%29%20Visualisierung%20als%20Streudiagram%0Aggplot%28data%2C%20aes%28x%3DSepal.Length%2C%20y%3DSepal.Width%2C%20color%3DSpecies%29%29%20%2B%0A%20%20geom_point%28%29%20%2B%0A%20%20labs%28title%3D%27Sepal%20length%20as%20a%20function%20of%20sepal%20width%27%2C%20%0A%20%20%20%20%20%20%20x%3D%27Sepal%20length%27%2C%20y%3D%27Sepal%20width%27%29), um die Ergebnisse zu sehen.


### Python-Skript

Und das Gleiche als Python-Skript:

```python
import pandas as pd
import matplotlib.pyplot as plt
# (1) Daten einlesen (ersetzt Umformatierung und Kopfzeilenentfernung)
data = pd.read_csv('https://zenodo.org/record/1319069/files/iris.csv')
# (2) Enthaltene Arten extrahieren
print(data['Species'].unique())
# (3) Anzahl der Datens√§tze pro Art bestimmen
print(data['Species'].value_counts())
# (4) Gruppieren pro Art und Mittelwerte der Spalten berechnen
grouped_data = data.groupby('Species').mean()
print(grouped_data)
# (5) Visualisierung als Streudiagramm
plt.scatter(x=data['Sepal.Length'], y=data['Sepal.Width'], c=data['Species'].astype('category').cat.codes)
plt.title('Sepal length as a function of sepal width')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.show()
# PS: Legende f√ºr die Farben fehlt noch...
```

:::::::::::::::


Nicht alle Schritte der Datenverarbeitung lassen sich immer einfach in Skripte √ºbersetzen.
H√§ufig sind mehrere Schritte notwendig, um eine vollst√§ndige Analyse durchzuf√ºhren, welche ggf. dann in verschiedenen Skriptdateien organisiert werden m√ºssen.
Allerdings ist nun die √ºbergeordnete Ausf√ºhrteihenfolge der Einzelskripte entscheidend, um die gesamte Analyse durchzuf√ºhren.
Das hei√üt, es entsteht eine **Abh√§ngigkeit** zwischen den einzelnen Skripten und die Organisations- und Dokumentationsprobleme verschieben sich nun auf die Ebene der **Verwaltung und Ausf√ºhrung von Skript-Abfolgen**.

An dieser Stelle kommen **Workflow-Systeme** und **Pipelines** ins Spiel, die genau diese Probleme adressieren.


## Workflow-Systeme und Pipelines

*Workflow-Systeme* sind spezialisierte Softwarel√∂sungen, die entwickelt wurden, um komplexe Datenverarbeitungsprozesse zu automatisieren, zu verwalten und zu dokumentieren.
Sie erm√∂glichen es Forschenden, verschiedene Schritte der Datenverarbeitung zu definieren, zu verkn√ºpfen und auszuf√ºhren, ohne dass sie sich um die technischen Details der Ausf√ºhrung k√ºmmern m√ºssen.
Workflows bestehen aus einer Reihe von **Schritten** oder **Tasks**, die nacheinander oder parallel ausgef√ºhrt werden k√∂nnen.
Jeder Schritt kann ein Skript, ein Programm oder ein Tool sein, das eine spezifische Aufgabe erf√ºllt.

*Pipelines* sind eine spezielle Art von Workflows, die sich auf die Verarbeitung von Daten in einer sequenziellen Abfolge von Schritten konzentrieren.
Pipelines sind besonders n√ºtzlich, wenn Daten durch mehrere Verarbeitungsschritte transformiert werden m√ºssen, bevor sie analysiert oder visualisiert werden k√∂nnen.
Das Konzept der Pipeline-basierten Datenverarbeitung kommt auch in vielen Skriptsprachen vor. Hierbei erlaubt ein spezieller Pipe-Operator die Ausgabe eines Befehls direkt als Eingabe in den n√§chsten Befehl weiterzureichen (z.B. `|` in Bash/Shell oder `|>` in R, s.o.).


Beide Konzepte, Workflows und Pipelines, bieten mehrere Vorteile:

- **Automatisierung**: Einmal definierte Workflows k√∂nnen automatisch ausgef√ºhrt werden, was den manuellen Aufwand reduziert und die Effizienz erh√∂ht.
- **Dokumentation**: Workflows dokumentieren die Abfolge der Verarbeitungsschritte, was die Nachvollziehbarkeit und Reproduzierbarkeit der Analyse verbessert.
- **Wiederverwendbarkeit**: Einmal erstellte Workflows k√∂nnen leicht angepasst und f√ºr √§hnliche Analysen wiederverwendet werden.
- **Fehlerbehandlung**: Viele Workflow-Systeme bieten Mechanismen zur Fehlererkennung und -behandlung, was die Robustheit der Datenverarbeitung erh√∂ht.
- **Skalierbarkeit**: Workflows k√∂nnen auf verschiedenen Systemen ausgef√ºhrt werden, von lokalen Rechnern bis hin zu Hochleistungsrechnern und Cloud-Umgebungen.

Hier sind einige verbreitete Workflow-Systeme und Tools zur Erstellung von Pipelines: 

- **Snakemake**: Ein Workflow-Management-System, das auf Python basiert und sich besonders f√ºr bioinformatische Analysen eignet.
- **Nextflow**: Ein weiteres beliebtes Workflow-Tool, das die Ausf√ºhrung von verschiedenen Umgebungen unterst√ºtzt, einschlie√ülich Cloud-Computing.
- **Galaxy**: Eine webbasierte Plattform, die es erm√∂glicht, Datenanalysen √ºber eine grafische Benutzeroberfl√§che durchzuf√ºhren und Workflows zu erstellen, ohne dass Programmierkenntnisse erforderlich sind.

Es gibt aber auch einfache Workflow-Systeme f√ºr spezielle Anwendungsf√§lle, wie z.B. **OpenRefine** f√ºr die Datenbereinigung und -transformation.

Im Folgenden werden wir uns beispielhaft zwei dieser Tools genauer anschauen: Galaxy und OpenRefine.


### Galaxy

Die [Galaxy](https://galaxyproject.org/)-Plattform ist ein webbasiertes Workflow-System, das speziell f√ºr die Analyse von wissenschaftlichen Daten entwickelt wurde, prim√§r im Bereich der Life-Sciences.
Galaxy bietet eine benutzerfreundliche grafische Oberfl√§che, die es Forschenden erm√∂glicht, komplexe Datenanalysen durchzuf√ºhren, ohne dass sie Programmierkenntnisse ben√∂tigen.
Die Plattform unterst√ºtzt eine Vielzahl von Datenformaten und Analysewerkzeugen, die in sogenannten "Tools" organisiert sind.

[![Galaxy Benutzeroberfl√§che (Wikipedia, click to enlarge)](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Galaxy_Workflow_Run_tuberculosis_tutorial_2025_03_v3.png/1920px-Galaxy_Workflow_Run_tuberculosis_tutorial_2025_03_v3.png){width=80% alt="Galaxy Benutzeroberfl√§che"}](https://en.wikipedia.org/wiki/Galaxy_%28computational_biology%29#/media/File:Galaxy_Workflow_Run_tuberculosis_tutorial_2025_03_v3.png)

Galaxy erm√∂glicht es Nutzern, Workflows zu erstellen, indem sie verschiedene Tools miteinander verkn√ºpfen.
Diese Workflows k√∂nnen gespeichert, geteilt und wiederverwendet werden, was die Zusammenarbeit und Reproduzierbarkeit von Analysen f√∂rdert.
Ein weiterer Vorteil von Galaxy ist die M√∂glichkeit, Analysen in einer Cloud-Umgebung durchzuf√ºhren, was die Skalierbarkeit und den Zugriff auf leistungsf√§hige Rechenressourcen erleichtert.

Es gibt ein einf√ºhrendes [Tutorial f√ºr einen Workflow in Galaxy](https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101-everyone/tutorial.html), der die Schritte aus unserem Iris-Datensatz-Beispiel automatisiert.
Die darin enthaltenen Arbeitsschritte und ihre Verkn√ºpfung zu einem Workflow k√∂nnen in Galaxy direkt nachvollzogen und visualisiert werden:

[![Galaxy Workflow des Iris-Beispiels (click to enlarge)](https://training.galaxyproject.org/training-material/topics/introduction/images/101_foreveryone_workflow_editor.png){width=80% alt="Galaxy Workflow"}](https://training.galaxyproject.org/training-material/topics/introduction/images/101_foreveryone_workflow_editor.png)

[Der Workflow selbst](https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101-everyone/workflows/main_workflow.html), ist dabei exportierbar und kann in anderen Galaxy-Instanzen importiert und ausgef√ºhrt werden.

Da Galaxy webbasiert ist, ist keine Installation auf dem eigenen Rechner notwendig.
Daten k√∂nnen direkt im Browser hochgeladen und analysiert werden.
Allerdings ben√∂tigt das System einen Server, auf dem es l√§uft und einiges an Rechenleistung und Administration.
Daher werden Galaxy-Instanzen h√§ufig von Institutionen f√ºr ihre Nutzer bereitgestellt.
Es gibt aber auch √∂ffentliche Galaxy-Server, die von jedem genutzt werden k√∂nnen, z.B. [usegalaxy.org](https://usegalaxy.org/).


### OpenRefine

[OpenRefine](https://openrefine.org/) (fr√ºher Google Refine) ist eine Open-Source Desktop-Anwendung zur Datenbereinigung und -transformation, die lokal auf dem eigenen Rechner installiert wird.
Sie bietet eine grafische Benutzeroberfl√§che, in der man gro√üe Datens√§tze importieren, bereinigen und untersuchen kann.

![OpenRefine Benutzeroberfl√§che](fig/openrefine-gui.png){width=80% alt="OpenRefine Benutzeroberfl√§che"}

OpenRefine ist besonders n√ºtzlich f√ºr die Arbeit mit unstrukturierten oder semi-strukturierten Daten, wie z.B. CSV-Dateien, Excel-Tabellen oder JSON-Daten.
Daten k√∂nnen hiermit einfach gefiltert, sortiert, gruppiert und transformiert werden.
Es unterst√ºtzt auch die Anwendung von regul√§ren Ausdr√ºcken und verwendet eine eigene Skriptsprache namens GREL (General Refine Expression Language) f√ºr komplexe Datenmanipulationen, welche √Ñhnlichkeiten zu JavaScript aufweist.

Alle Daten und durchgef√ºhrten Schritte werden in OpenRefine in einem sogenannten "Projekt" gemeinsam verwaltet, das exportiert und sp√§ter wieder importiert werden kann.
Dies erm√∂glicht es, die Datenbereinigung nachvollziehbar zu dokumentieren und auf √§hnliche Datens√§tze anzuwenden.



## Portierbarkeit

Die Portierbarkeit von Datenverarbeitungsprozessen bezieht sich auf die F√§higkeit, diese Prozesse auf verschiedenen (Betriebs)Systemen und Umgebungen (Computern) auszuf√ºhren, ohne dass umfangreiche Anpassungen erforderlich sind.
Dies ist besonders wichtig in der wissenschaftlichen Forschung, wo Datenanalysen oft auf unterschiedlichen Rechnern, in verschiedenen Institutionen oder in Cloud-Umgebungen durchgef√ºhrt werden m√ºssen.
Oder wenn verschiedene Teammitglieder mit unterschiedlichen Systemen arbeiten.

Skriptsprachen erm√∂glichen h√§ufige eine hohe Portierbarkeit, da sie plattformunabh√§ngig sind und auf verschiedenen Betriebssystemen (Windows, macOS, Linux) ausgef√ºhrt werden k√∂nnen.
Hierbei muss auf dem jeweiligen System lediglich die entsprechende Laufzeitumgebung (der Interpreter) installiert sein.
Das hei√üt, ein einmal geschriebenes Skript kann in der Regel ohne √Ñnderungen auf verschiedenen Systemen ausgef√ºhrt werden, solange die ben√∂tigten Abh√§ngigkeiten (Bibliotheken, Pakete) ebenfalls verf√ºgbar sind.

Allerdings k√∂nnen Unterschiede in den Systemumgebungen, wie z.B. unterschiedliche Versionen von Bibliotheken oder (Betriebs)System-spezifische Pfade, die Portierbarkeit beeintr√§chtigen.

Workflow-Systeme wie Galaxy sind ebenfalls darauf ausgelegt, portabel zu sein, da die erzeugten Workflows umfangreiche Metadaten √ºber die verwendeten Tools und Arbeitsschritte enthalten, die die Ausf√ºhrung auf verschiedenen Instanzen erm√∂glichen.
Hierbei liegt ein deutlicher Fokus darauf, das Workflows m√∂glichst unabh√§ngig von der zugrunde liegenden Infrastruktur sind und somit archiviert oder ver√∂ffentlicht werden k√∂nnen.

Ein weiterer Ansatz zur Verbesserung der Portierbarkeit ist die Verwendung von Containerisierungstechnologien wie Docker oder Singularity.
Container erm√∂glichen es, eine komplette Softwareumgebung, einschlie√ülich aller Abh√§ngigkeiten und Konfigurationen, in einem isolierten Paket zu b√ºndeln.
Dies stellt sicher, dass die Datenverarbeitungsprozesse in der gleichen Umgebung ausgef√ºhrt werden, unabh√§ngig vom Host-System.
Container sind besonders n√ºtzlich, wenn komplexe Software-Stacks ben√∂tigt werden, die schwer zu installieren oder zu konfigurieren sind.



## Skalierbarkeit

Skalierbarkeit bezieht sich auf die F√§higkeit eines Systems, einer Anwendung oder eines Prozesses, mit zunehmender Datenmenge oder Arbeitslast umzugehen, ohne dass die Leistung oder Effizienz erheblich beeintr√§chtigt wird.
Im Bezug auf Datenverarbeitung bedeutet dies, dass die Methoden und Werkzeuge, die zur Analyse von Daten verwendet werden, in der Lage sein sollten, gr√∂√üere Datens√§tze entsprechend zu verarbeiten.

Skriptsprachen bieten in der Regel eine gute Skalierbarkeit, da sie auf leistungsf√§higen Computern oder Servern ausgef√ºhrt werden k√∂nnen, die √ºber ausreichende Ressourcen verf√ºgen, um gro√üe Datenmengen zu verarbeiten.
Das hei√üt, ein Skript, das auf einem lokalen Rechner mit einem kleinen Datensatz funktioniert, kann oft auch auf einem Hochleistungsrechner oder in einer Cloud-Umgebung ausgef√ºhrt werden, um gr√∂√üere Datens√§tze zu analysieren.
Allerdings k√∂nnen Skripte, die f√ºr kleine Datens√§tze optimiert sind, bei gr√∂√üeren Datenmengen ineffizient werden, wenn sie nicht entsprechend angepasst oder optimiert werden.

Workflow-Systeme wie Galaxy sind ebenfalls darauf ausgelegt, skalierbar zu sein.
Hierbei liegt der Fokus auf der Verwaltung der Ausf√ºhrung von Workflows auf Hochleistungsrechnern und Cloud-Umgebungen.
Die zugrundelegenden Queue- und Ressourcenmanagement-Systeme erm√∂glichen es, Workflows effizient zu verteilen und auszuf√ºhren, auch wenn die Datenmengen erheblich zunehmen.
Dabei werden die einzelnen Schritte eines Workflows oft parallelisiert, um die Verarbeitungsgeschwindigkeit zu erh√∂hen und die Auslastung der verf√ºgbaren Ressourcen zu optimieren.
Diese Verwaltung geschieht dabei automatisch, ohne dass der Nutzer sich um die technischen Details k√ºmmern muss.


üí¨ 8. Diskussion & Ausblick
  - Welche Tools nutzen die Studierenden bereits?
  - Welche Herausforderungen sehen sie in der Datenverarbeitung?
  - Ausblick auf weiterf√ºhrende Themen: Datenbanken, Big Data, Machine Learning

9. Clustercomputing etc...






https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101-everyone/tutorial.html
https://training.galaxyproject.org/training-material/topics/introduction/tutorials/galaxy-intro-101-everyone/workflows/main_workflow.html



# Galaxy 101 Workflow ‚Äì Zusammenfassung

## üéØ Ziel
Demonstration eines einfachen, reproduzierbaren Datenanalyse-Workflows mit Galaxy anhand des Iris-Datensatzes.

---

## üß∞ Workflow-Schritte

### 1. Datenimport
- **Tool**: `Upload Data`
- **Datei**: `iris.csv`
- **Ziel**: Laden des Iris-Datensatzes in Galaxy

### 2. CSV in tabellarisches Format umwandeln
- **Tool**: `csv_to_tabular`
- **Ziel**: Konvertierung der CSV-Datei in ein tabellarisches Format f√ºr weitere Verarbeitung

### 3. Entfernen der Kopfzeile
- **Tool**: `Remove beginning`
- **Parameter**: Entferne die erste Zeile
- **Ziel**: Entfernen der Spalten√ºberschriften

### 4. Spalten extrahieren
- **Tool**: `Cut columns from a table`
- **Parameter**: Auswahl relevanter Spalten (z.‚ÄØB. Sepal Length, Petal Length, Species)
- **Ziel**: Reduktion auf relevante Daten

### 5. Gruppierung und Aggregation
- **Tool**: `Datamash`
- **Parameter**: Gruppieren nach Spezies, Berechnung von Mittelwerten
- **Ziel**: Statistische Analyse der Daten

### 6. Sortieren und Duplikate entfernen
- **Tool**: `Sort`, `Unique`
- **Ziel**: Aufbereitung der Daten f√ºr Visualisierung

### 7. Visualisierung
- **Tool**: `ggplot2`
- **Parameter**: Erstellen eines Scatterplots (z.‚ÄØB. Sepal vs. Petal L√§nge, farblich nach Spezies)
- **Ziel**: Grafische Darstellung der Daten

---

## üì¶ Outputs
- Bereinigte und aggregierte Datentabellen
- Interaktive Visualisierungen (ggplot2-Plots)

---

## üß† Lernziele
- Nutzung von Galaxy als grafisches Workflow-System
- Verarbeitung tabellarischer Daten ohne Programmierung
- Erstellung und Wiederverwendung von Workflows
- Visualisierung von Daten direkt im Browser












## Zusammenfassung

:::::: keypoints

- 
  
::::::::::::::::


:::::::::: challenge

# Einordnung im Datenlebenszyklus

![*In welchen Phasen im Datenlebenszyklus sehen sie obige Punkte als besonders wichtig an?*](https://uni-tuebingen.de/fileadmin/_processed_/6/b/csm_FDM_Lebenszyklus_d1353825c4.png){width=40% alt="Datenlebenszyklus"}

::: solution

## Antwort

Das Wissen um die **digitale Repr√§sentation von Informationen** ist immer dann zentral, wenn Daten importiert oder exportiert werden, um sicherzustellen, dass die Daten korrekt √ºbertragen und gespeichert werden.

- **Planung**: Festlegung von Datenformaten und -strukturen
- **Erhebung**: Korrekte Interpretation von Daten
- **Archivierung**: Export von Daten in geeigneten Formaten
- **Nachnutzung**: korrekter Datenimport und -export

:::

::::::::::::::::::::



:::::::::::::::: instructor

## Aufgaben

### Sitzungsaufgaben

- 

### Sitzungsfragen

- 

:::::::::::::::::::::::::::

